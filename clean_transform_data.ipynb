{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read comment data\n",
    "all_comments = pd.read_csv('2021-2022/new/comments.csv', index_col=0)\n",
    "all_comments['created_utc'] = pd.to_datetime(all_comments['created_utc'],unit='s')\n",
    "all_comments.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NA\n",
    "all_comments['body'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we do anything we should also convert Markdown to plain text\n",
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "\n",
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text)\n",
    "\n",
    "\n",
    "# Remove all emojis (I might need to get keep that data in the future for something, but not this project)\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove everything that we don't need\n",
    "all_comments['body'] = all_comments['body'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "all_comments['body'] = all_comments.apply(lambda x: unmark(x['body']), axis = 1)\n",
    "all_comments['body'] = all_comments['body'].str.replace('&#x200B;>', ' ')\n",
    "all_comments['body'] = all_comments.apply(lambda x: remove_emoji(x['body']), axis = 1)\n",
    "all_comments['body'] = all_comments['body'].str.replace(r\"\\\\n\", \"\", regex=True)\n",
    "# Also remove any comment that has been deleted\n",
    "all_comments = all_comments[all_comments[\"body\"].str.contains(\"deleted\")==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Bulgarian stop words\n",
    "from spacy.lang.bg.stop_words import STOP_WORDS as BG_STOPWORDS\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "# Modules for word2vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "## First quick and drity attempt at getting n-grams\n",
    "stopwords_bg = 'на от за да се по са ще че не това си като до през които най при но има след който към бъде той още може му което много със която или само тази те обаче във вече около както над така между ако лв им тези преди млн бе също пред ни когато защото кв би пък тъй ги ли пак според този все някои'\n",
    "stopwords_custom = stopwords_bg.split()\n",
    "stopwords_custom.append('не')\n",
    "\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = list(BG_STOPWORDS) + stopwords_custom\n",
    "\n",
    "def prepare_ngram(text, ngrams):\n",
    "    # Take punctuations out\n",
    "    cleaned_string = re.sub('\\[,.*?“”…\\]', '', text)\n",
    "    cleaned_string = re.sub(r'[“”]', '', cleaned_string)\n",
    "\n",
    "    # Remove any digits\n",
    "    cleaned_string = ''.join([i for i in cleaned_string if not i.isdigit()])\n",
    "\n",
    "    # Tokenise the data\n",
    "    cleaned_string = re.sub('[%s]' % re.escape(string.punctuation), ' ', cleaned_string)\n",
    "\n",
    "    # Why did I needed to do lowercase? Double-check that - it doesn't match Google Ngram behavior\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    TOKENS = word_tokenize(cleaned_string) \n",
    "\n",
    "    # Filter those stop words out\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in TOKENS: \n",
    "        if w not in ADDITIONAL_STOPWORDS:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    # Count phrases\n",
    "    gram_df = pd.Series(nltk.ngrams(filtered_sentence, ngrams)).value_counts()\n",
    "    return gram_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dataset for unigrams\n",
    "full_df_unigram = all_comments.groupby('created_utc').apply(lambda x: prepare_ngram(x['body'].str.cat(sep=', '), 1))\n",
    "full_df_unigram = full_df_unigram.reset_index().rename(columns={'created_utc' : 'date', 'level_1' : 'unigram', 0 : 'count'})\n",
    "full_df_unigram = full_df_unigram.explode('unigram')\n",
    "\n",
    "# Remove all strings that are single characters and find out why they are there in the first place?!\n",
    "def remove_single_char(string):\n",
    "    return string.count(string[0]) == len(string)\n",
    "\n",
    "full_df_unigram['drop_value'] = full_df_unigram.apply(lambda x: remove_single_char(x['unigram']), axis = 1)\n",
    "full_df_unigram = full_df_unigram[full_df_unigram['drop_value'] == False]\n",
    "full_df_unigram = full_df_unigram.drop(columns=['drop_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same with full dataset for bigrams\n",
    "full_df_bigram = all_comments.groupby('created_utc').apply(lambda x: prepare_ngram(x['body'].str.cat(sep=', '), 2))\n",
    "full_df_bigram = full_df_bigram.reset_index().rename(columns={'created_utc' : 'date', 'level_1' : 'bigram', 0 : 'count'})\n",
    "full_df_bigram['bigram'] = full_df_bigram.bigram.apply(lambda x: ' '.join([str(i) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column name to be concatanate two dataframes easier \n",
    "full_df_bigram = full_df_bigram.rename(columns={'bigram' : 'unigram'})\n",
    "\n",
    "full_df_unigram['unigram'] = full_df_unigram['unigram'].str.replace('\\W+', '', regex=True)\n",
    "full_df_unigram['unigram'] = full_df_unigram['unigram'].replace('', np.nan)\n",
    "full_df_unigram.dropna(subset=['unigram'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatanate unigram and bigram\n",
    "full_df = pd.concat([full_df_unigram, full_df_bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare monthly counts and proportions for the combined dataframe (we want to be able to compare\n",
    "# unigrams to bigrams directly)\n",
    "full_df.set_index('date', inplace=True)\n",
    "full_df.index = pd.DatetimeIndex(full_df.index)\n",
    "monthly_full = full_df.groupby([pd.Grouper(freq=\"M\"), \"unigram\"]).sum().reset_index()\n",
    "monthly_full['ratio'] = (monthly_full.groupby(['unigram','date'])['count'].transform(sum) / monthly_full.groupby('date')['count'].transform(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/bk8xzzt10l30g9c3zpfjjh700000gn/T/ipykernel_77348/3323305685.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  monthly_full_2016['date'] = monthly_full_2016['date'] - pd.offsets.MonthBegin(1, normalize=True)\n"
     ]
    }
   ],
   "source": [
    "# Get only data with enough datapoints for plotting\n",
    "monthly_full_2016 = monthly_full[monthly_full['date'] >= \"2016-01-31\"]\n",
    "\n",
    "# Normalize dates to begining of month\n",
    "monthly_full_2016['date'] = monthly_full_2016['date'] - pd.offsets.MonthBegin(1, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that checks whether the string is 1 or 2-gram. \n",
    "# so that we load it from appropriate db. For the time being not planning to do more than bigram\n",
    "\n",
    "def check_string(string):\n",
    "    ngram = len(string.split())\n",
    "    if ngram == 1:\n",
    "        dataframe = 'unigram'\n",
    "    elif ngram == 2:\n",
    "        dataframe = 'bigram'\n",
    "    else:\n",
    "        print(\"We don't support more than that currently.\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_full_2016 = monthly_full_2016.rename(columns={'unigram' : 'gram'})\n",
    "\n",
    "# Let's brake the big df into two smaller ones (should be faster in production)\n",
    "monthly_full_2016['dataframe'] = monthly_full_2016.apply(lambda x: check_string(x['gram']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframes back to separate unigram and bigram\n",
    "a = monthly_full_2016['dataframe'] == 'unigram'\n",
    "unigram_full_df = monthly_full_2016[a]\n",
    "bigram_full_df = monthly_full_2016[~a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_full_df.to_csv(\"data/unigram_full_df.csv\")\n",
    "bigram_full_df.to_csv(\"data/bigram_full_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('monitoring')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78e575c1de9f0cd6562fc7bad27c621d628df2c8e472590f97d49c7548e9665f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
